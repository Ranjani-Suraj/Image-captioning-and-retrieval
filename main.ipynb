{"cells":[{"cell_type":"code","source":["%pip install -r requirements.txt"],"metadata":{"id":"eVawWZPIe3Mn"},"id":"eVawWZPIe3Mn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","\n","import dataset # MiniFlickrDataset, get_loader\n","from captioner import CaptioningModel\n","from train_caption import Trainer\n","from lr_warmup import LRWarmup\n","import utils\n","from data_loader import get_loader\n","from utils import Ranker\n","from retriever import Model, Criterion, train, val, save_ckp_rt\n"],"metadata":{"id":"5LfnPJ5gez2_"},"id":"5LfnPJ5gez2_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"58a5cd39","metadata":{"id":"58a5cd39"},"outputs":[],"source":["!unzip images.zip -d images/data/"]},{"cell_type":"code","execution_count":null,"id":"7b543780","metadata":{"id":"7b543780"},"outputs":[],"source":["\n","class Args_caption():\n","    def __init__(self):\n","        self.data_path = 'dataset.pkl'\n","        self.clip_model = 'openai/clip-vit-base-patch32'\n","        self.text_model = 'gpt2'\n","        self.seed = 100\n","        self.num_workers = 0\n","        self.train_size = 0.84\n","        self.val_size = 0.13\n","        self.test_size = 100\n","        self.epochs = 10\n","        self.lr = 3e-3\n","        self.k = 0.33\n","        self.batch_size_exp = 6\n","        self.ep_len = 4\n","        self.num_layers = 6\n","        self.n_heads = 16\n","        self.forward_expansion = 4\n","        self.max_len = 40\n","        self.dropout = 0.1\n","config = Args_caption()"]},{"cell_type":"code","execution_count":null,"id":"6263d09e","metadata":{"id":"6263d09e"},"outputs":[],"source":["\n","class Args_retrieval():\n","    def __init__(self):\n","        # Dataset\n","        self.data_root = \"./images/data/\"\n","        self.data_set = \"dress\"\n","        self.image_root = os.path.join(self.data_root, 'resized_images/')\n","        self.caption_path = os.path.join(self.data_root, 'images/data/captions/captions/cap.{}.{}.json')\n","        self.split_path = os.path.join(self.data_root, 'images/data/image_splits/image_splits/split.{}.{}.json')\n","\n","        # Model\n","        self.embed_dim = 512\n","        self.vision_feature_dim = 512\n","        self.text_feature_dim = 512\n","\n","        # Training\n","        self.log_step = 15\n","        self.batch_size = 64\n","        self.learning_rate = 0.001\n","        self.num_workers = 4\n","        self.epochs = 3\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","args = Args_retrieval()\n"]},{"cell_type":"code","execution_count":null,"id":"e5a99f0f","metadata":{"id":"e5a99f0f"},"outputs":[],"source":["#setup training for captioning model\n","\n","device = utils.init_env(config.seed)\n","\n","# Create data loaders\n","dataset = dataset.MiniFlickrDataset(config.data_path)\n","config.train_size = int(config.train_size * len(dataset))\n","config.val_size = len(dataset) - config.train_size - config.test_size\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [config.train_size, config.val_size, config.test_size])\n","train_loader = dataset.get_loader(\n","    train_dataset,\n","    bs_exp=config.batch_size_exp,\n","    shuffle=True,\n","    num_workers=config.num_workers,\n","    pin_memory=True,\n",")\n","test_loader = dataset.get_loader(\n","    test_dataset,\n","    bs_exp=0,\n","    shuffle=False,\n","    num_workers=config.num_workers,\n","    pin_memory=True,\n","    train=False,\n",")\n","\n","# Creat model\n","capt_model = CaptioningModel(\n","    clip_model=config.clip_model,\n","    text_model=config.text_model,\n","    ep_len=config.ep_len,\n","    num_layers=config.num_layers,\n","    n_heads=config.n_heads,\n","    forward_expansion=config.forward_expansion,\n","    dropout=config.dropout,\n","    max_len=config.max_len,\n","    device=device\n",")\n","\n","# Create optimizer, lr scheduler\n","optimizer = optim.Adam(capt_model.parameters(), lr=config.lr)\n","warmup = LRWarmup(epochs=config.epochs, max_lr=config.lr, k=config.k)\n","scheduler = optim.lr_scheduler.LambdaLR(optimizer, warmup.lr_warmup)\n","\n","# Create trainer\n","trainer = Trainer(\n","    model=capt_model,\n","    optimizer=optimizer,\n","    scaler=torch.amp.GradScaler('cuda'),\n","    scheduler=scheduler,\n","    train_loader=train_loader,\n","    test_loader=test_loader,\n","    device=device\n",")\n","\n","# use _load_ckpt method of the trainer to load weights from the saved checkpoint to resume the training. Below is a sample code for the same\n","\n","#trainer._load_ckp(\"path to .pt file\")\n"]},{"cell_type":"code","execution_count":null,"id":"19667006","metadata":{"id":"19667006"},"outputs":[],"source":["# Start training\n","for epoch in range(trainer.epoch, config.epochs):\n","    trainer.train_epoch()\n","\n","    score = trainer.test_epoch()\n","    print(\"Score: {:.4f}\".format(score))\n","\n","    os.makedirs(\"checkpoints\", exist_ok=True)\n","    if (epoch + 1) % 3 == 0:\n","        trainer.save_ckp(os.path.join(\"checkpoints\", f'epoch_{epoch + 1}.pt'))\n"]},{"cell_type":"code","execution_count":null,"id":"bb0c5818","metadata":{"id":"bb0c5818"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f4be03e7","metadata":{"id":"f4be03e7"},"outputs":[],"source":["!python resize_images.py --image_dir images/data/images/data/ --output_dir images/data/resized_images --image_size 256"]},{"cell_type":"code","execution_count":null,"id":"ed969839","metadata":{"id":"ed969839"},"outputs":[],"source":["#setup training for retrieval model\n","# Build data loader\n","data_loader = get_loader(\n","    args.image_root.format(args.data_set),\n","    args.caption_path.format(args.data_set, 'train'),\n","    args.batch_size,\n","    shuffle=True,\n","    return_target=True,\n","    num_workers=args.num_workers,\n",")\n","data_loader_dev = get_loader(\n","    args.image_root.format(args.data_set),\n","    args.caption_path.format(args.data_set, 'val'),\n","    args.batch_size,\n","    shuffle=False,\n","    return_target=True,\n","    num_workers=args.num_workers,\n",")\n","#images\\data\\images\\data\\captions\\captions\\cap.dress.train.json\n","# Build model, criterion, oprimizer, evaluator\n","ret_model = Model(args.vision_feature_dim, args.text_feature_dim, args.embed_dim)\n","ret_model.to(args.device)\n","ret_model.train()\n","criterion = Criterion()\n","current_lr = args.learning_rate\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, ret_model.parameters()), lr=current_lr)\n","ranker = Ranker(root=args.image_root.format(args.data_set),\n","                image_split_file=args.split_path.format(args.data_set, 'val'),\n","                transform=None, num_workers=args.num_workers)"]},{"cell_type":"code","execution_count":null,"id":"a8a8b385","metadata":{"id":"a8a8b385"},"outputs":[],"source":["# Start training\n","best_score = 0\n","for epoch in range(args.epochs):\n","\n","    train(data_loader, ret_model, criterion, optimizer, args.log_step)\n","    best_score = val(data_loader_dev, ret_model, ranker, best_score)\n","\n","save_ckp_rt(ret_model, os.path.join(\"checkpoint\", f'epoch_{-1}.pt'))\n","print(best_score)"]},{"cell_type":"code","execution_count":null,"id":"d60add1d","metadata":{"id":"d60add1d"},"outputs":[],"source":["#generate a caption from an image using the trained captioning model\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import io\n","def generate_caption(capt_model, images):\n","\n","        num_examples = len(images)\n","        capt_model.model.eval()\n","\n","        fig, axs = plt.subplots(num_examples, 1, figsize=(20, 12))\n","        captions = list()\n","        for idx, img in enumerate(images):\n","            # img_path = image_paths[idx]\n","            # img = Image.open(img_path)\n","\n","            with torch.no_grad():\n","                caption, _ = capt_model.model(img)\n","\n","            axs[idx].imshow(img)\n","            axs[idx].set_title(caption)\n","            captions.append(caption)\n","        buf = io.BytesIO()\n","        plt.savefig(buf, format=\"png\")\n","        buf.seek(0)\n","        plt.show()\n","        fig.clear()\n","        plt.close(fig)\n","        return captions\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5a3e83e0","metadata":{"id":"5a3e83e0"},"outputs":[],"source":["#get image from caption\n","def get_image_from_caption(ranker, caption):\n","    ret_model.eval()\n","    with torch.no_grad():\n","        image = ranker.retrieve_image(ret_model, caption)\n","    return image\n","\n","def get_caption_from_image(ranker, image_path):\n","    ret_model.eval()\n","    with torch.no_grad():\n","        caption = ranker.retrieve_caption(ret_model, image_path)\n","    return caption"]},{"cell_type":"code","execution_count":null,"id":"251aa19e","metadata":{"id":"251aa19e"},"outputs":[],"source":["#from an input image, retrieve a caption and generate a caption for the retrieved image\n","input_image = \"path/to/your/image.jpg\"\n","img = Image.open(input_image).convert(\"RGB\")\n","retrieved_caption = get_caption_from_image(ranker, input_image)\n","caption = generate_caption(capt_model, [img])\n","print(\"captions: \", retrieved_caption, caption)"]},{"cell_type":"code","execution_count":null,"id":"4e510e5c","metadata":{"id":"4e510e5c"},"outputs":[],"source":["# from an input caption, retrieve an image and generate a caption for the retrieved image\n","input_caption = \"A blue dress with white polka dots\"\n","retrieved_image = get_image_from_caption(ranker, input_caption)\n","plt.imshow(np.array(retrieved_image))\n","caption = generate_caption(capt_model, [retrieved_image])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}